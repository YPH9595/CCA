{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer,BertModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#bert = BertModel.from_pretrained('dmis-lab/biobert-large-cased-v1.1')\n",
    "#tokenizer = BertTokenizer.from_pretrained('dmis-lab/biobert-large-cased-v1.1')\n",
    "\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "\n",
    "## YOU MAY CHANGE THESE HYPERPARAMETERS\n",
    "LABEL_NUM = 5\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 1e-5\n",
    "DROPOUT_RATIO = 0 \n",
    "MAX_EPOCH = 5\n",
    "TEST_TRAIN_RATIO = 0.3\n",
    "MAX_LEN = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## if you want to use TPU on Colab, run this code\n",
    "\n",
    "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8.1-cp37-cp37m-linux_x86_64.whl\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "\n",
    "device = xm.xla_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = pd.read_csv('train.dat', sep = '\\t', header=None)\n",
    "df_train, df_test = train_test_split(text_data, test_size=TEST_TRAIN_RATIO)\n",
    "df_train.reset_index(drop=True, inplace = True)\n",
    "df_test.reset_index(drop=True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset for BERT\n",
    "\n",
    "### TODO : Preprocess input data\n",
    "\n",
    "#### example : when maximum length is 8\n",
    "\n",
    "- original input sentence - \"I really love you\"\n",
    "- tokenizing (use ``bert_tokenizer.tokenize``) -  ['i', 'really', 'love', 'you']\n",
    "- Add special token - ['[CLS]' 'i', 'really', 'love', 'you', '[SEP]'] (length = 6)\n",
    "- Add padding tokens to fit maximum length - ['[CLS]' 'i', 'really', 'love', 'you', '[SEP]', '[PAD]','[PAD]']\n",
    "- Convert tokens to id (use ``bert_tokenizer.convert_tokens_to_ids``)\n",
    "- make attention mask to tell which token is a padding token - [1,1,1,1,1,1,0,0]\n",
    "\n",
    "You may choose other way (even simpler) to preprocess the input text. see https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer or https://huggingface.co/transformers/model_doc/bert.html#berttokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClinicalDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, maxlen, tokenizer):\n",
    "\n",
    "        self.df = dataframe.rename(columns={0: \"label\", 1: \"text\"})\n",
    "        #Initialize the BERT tokenizer\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.df['label'] = self.df['label'].apply(lambda x : x-1)\n",
    "\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        #Selecting the sentence and label at the specified index in the data frame\n",
    "        sentence = self.df.loc[index, 'text']\n",
    "        label = self.df.loc[index, 'label']\n",
    "\n",
    "        ##TODO \n",
    "\n",
    "        return sequence, attention_mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating instances of training and validation set\n",
    "train_set = ClinicalDataset(df_train, maxlen = MAX_LEN,tokenizer = bert_tokenizer)\n",
    "val_set = ClinicalDataset(df_test, maxlen = MAX_LEN,tokenizer = bert_tokenizer)\n",
    "\n",
    "#Creating intsances of training and validation dataloaders\n",
    "train_loader = DataLoader(train_set, batch_size = BATCH_SIZE, num_workers = 2)\n",
    "val_loader = DataLoader(val_set, batch_size = BATCH_SIZE, num_workers = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Neural Network\n",
    "\n",
    "### TODO : Define layers and data flow \n",
    "\n",
    "``nn.Dropout`` could be used to prevent overffiting. (parameter : dropout ratio)\n",
    "\n",
    "from ``bert.config.to_dict()['hidden_size']``, we can obtain size of embedding vector used in pretrained BERT model.\n",
    "\n",
    "Here is information about pytorch functions that used for layers (e.g. ``nn.Linear``):\n",
    "https://pytorch.org/docs/stable/nn.html\n",
    "\n",
    "Here is more information about BERT model implementation on PyTorch : https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self,bert,output_len,dropout):\n",
    "        super(Classifier, self).__init__()\n",
    "        ##TODO\n",
    "        \n",
    "        ## Define Layers\n",
    "        ## \n",
    "\n",
    "    def forward(self, sequence, attention_masks):\n",
    "\n",
    "        ##TODO\n",
    "        \n",
    "        ##Define data flow in neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "#device = torch.device('cpu')\n",
    "net = Classifier(bert,LABEL_NUM,DROPOUT_RATIO)\n",
    "net.to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(output, labels):\n",
    "\n",
    "    _, pred = torch.max(output.data, axis=1)\n",
    "    ans = (pred == labels.squeeze()).sum()\n",
    "    \n",
    "    return ans\n",
    "\n",
    "def evaluate(net, criterion, dataloader):\n",
    "    net.eval()\n",
    "\n",
    "    mean_acc, mean_loss = 0, 0\n",
    "    count = 0\n",
    "    ans = 0\n",
    "    total_num = 0\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for seq, attn_masks, labels in dataloader:\n",
    "            seq, attn_masks, labels = seq.to(device), attn_masks.to(device), labels.to(device)\n",
    "            output = net(seq, attn_masks)\n",
    "            mean_loss += criterion(output.squeeze(-1), labels.long()).item()\n",
    "            \n",
    "            ans += get_accuracy(output, labels)\n",
    "            total_num += labels.size(0)\n",
    "            count += 1\n",
    "\n",
    "\n",
    "    return float(ans) / float(total_num), mean_loss / count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, criterion, optimizer, dataloader):\n",
    "    \n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    ans = 0\n",
    "    total_num = 0\n",
    "    net.train()\n",
    "        \n",
    "    for i, (sequence, attention_mask, labels) in enumerate(dataloader):\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()  \n",
    "        \n",
    "        sequence, attention_mask, labels = sequence.to(device), attention_mask.to(device), labels.to(device)\n",
    "        \n",
    "        output = net(sequence, attention_mask)\n",
    "        \n",
    "        loss = criterion(output.squeeze(-1), labels.long())\n",
    "        \n",
    "        ##loss.backward() calculate gradients of each parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        ##optimizer.step() updates learnable parameters in the NN using calculated gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += criterion(output.squeeze(-1), labels.long()).item()\n",
    "        ans += get_accuracy(output, labels)\n",
    "        total_num += labels.size(0)\n",
    "        count += 1\n",
    "        acc = float(ans)/ float(total_num)\n",
    "        mean_loss = float(total_loss/count)  \n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(\"Iteration {} complete. Loss : {} Accuracy : {}\".format(i+1, mean_loss, acc))\n",
    "     \n",
    "    return acc, mean_loss\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_list = []\n",
    "train_loss_list = []\n",
    "\n",
    "test_acc_list = []\n",
    "test_loss_list = []\n",
    "for epoch in range(MAX_EPOCH):\n",
    "    train_acc, train_loss = train(net,criterion,optimizer,train_loader)\n",
    "    test_acc, test_loss = evaluate(net,criterion,val_loader)\n",
    "    print(\"Epoch {} complete! Validation Loss : {} Validation Accuracy : {}\".format(epoch+1,test_loss,test_acc))\n",
    "    \n",
    "    train_acc_list.append(train_acc)\n",
    "    train_loss_list.append(train_loss)\n",
    "    \n",
    "    test_acc_list.append(test_acc)\n",
    "    test_loss_list.append(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(1,MAX_EPOCH+1,1)\n",
    "\n",
    "plt.plot(x,train_acc_list)\n",
    "plt.plot(x,test_acc_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nkhugh-anaconda",
   "language": "python",
   "name": "nkhugh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
